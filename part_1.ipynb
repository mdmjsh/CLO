{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import avg, max, sum\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from uszipcode import SearchEngine as ZipCodeEngine\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "BIG_TAXI = 's3a://chictaxi/chictaxi.csv'\n",
    "SMALL_TAXI = 's3a://chictaxi/small.csv'\n",
    "WEATHER = 's3a://chictaxi/weather.csv'\n",
    "\n",
    "# sc = SparkContext()\n",
    "sql_context = SQLContext(sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_data(sql_context, path=BIG_TAXI):\n",
    "    df = sql_context.read.csv(path, header='true', inferSchema='true')\n",
    "    return (df, df.rdd)\n",
    "\n",
    "\n",
    "def get(x, key, default=0):\n",
    "    return getattr(x, key) or default\n",
    "\n",
    "\n",
    "def string_to_time(date):\n",
    "    \"\"\"E.g. turns '04/13/2017 07:30:00 AM' into datetime.time(6, 15).    \n",
    "\n",
    "    N.b. extra complexity here as time format isn't a simple 24hr clock;\n",
    "    first convert to PM times to 24 hr format by manipulating the string, \n",
    "    then convert to DateTime.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'PM' in date:\n",
    "            time = date.split(' ')[1]\n",
    "            hour = str(int(date.split(':')[0].split(' ')[-1]) + 12)\n",
    "            _time = hour + time[2:]\n",
    "            _date = date.replace(time, _time)[:-3]\n",
    "\n",
    "        else:\n",
    "            _date = date[:-3]\n",
    "        # https://www.journaldev.com/23365/python-string-to-datetime-strptime\n",
    "        return datetime.strptime(_date, '%m/%d/%Y %H:%M:%S')\n",
    "    except ValueError:\n",
    "        date\n",
    "\n",
    "\n",
    "def test_string_to_time():\n",
    "    assert string_to_time('04/13/2017 07:30:00 PM') == datetime.datetime(2017, 4, 13, 19, 30)\n",
    "    assert string_to_time('04/13/2017 07:30:00 AM') == datetime.datetime(2017, 4, 13, 7, 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a3dbec78b8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtaxi_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaxi_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-ddf7e05053c0>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(sql_context, path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBIG_TAXI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msql_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "taxi_df, taxi_rdd = get_data(sql_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=rdd#pyspark.RDD.sample \n",
    "sampled_rdd = taxi_rdd.sample(False, 0.0001, 81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1(rdd):\n",
    "    \"\"\"How many taxi records are there? \n",
    "    How many taxi records for each year of the dataset?\n",
    "    \"\"\"\n",
    "    count = rdd.count() \n",
    "    yearly_counts = rdd.map(lambda x: (getattr(x, 'Trip Start Timestamp'\n",
    "                                              ).split('/')[-1].split(' ')[0], 1)).reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "    return count, yearly_counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1 answer\n",
    "# total_records, yearly_counts = q1(sampled_rdd)\n",
    "total_records, yearly_counts = q1(taxi_rdd)\n",
    "# yearly_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def q2(rdd, total_records):\n",
    "    \"\"\"\"How many records in total would you classify as bad? \n",
    "        Consider a bad record to be one where the Trip Seconds are less than 60,\n",
    "        but also if the average speed is over 100 mph, the distance is more \n",
    "        than 1000 miles or the fare is over $2000 (excluding tips, tolls, etc). \n",
    "        \n",
    "        Once you have defined this, ensure that all further answers are based only on good data. \n",
    "        How many records are “good” by year\n",
    "        \n",
    "        \n",
    "        N.b. for trips under 1 mile, the ave. speed = 0.0 (as miles = 0), therefore this is a decent approximation\n",
    "        without guarenteeing total accuracy as it doesn't take into account the precise coordinates of the journey\n",
    "        when calculating average speed.\n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    good_trips = rdd.filter(lambda x: (get(x, 'Trip Seconds') > 60 )\n",
    "                    & (get(x, 'Trip Miles') < 1000)\n",
    "                    & (get(x, 'Fare') < 2000))\n",
    "#                     & ((get(x, 'Trip Miles') / (get(x, 'Trip Seconds') / 60)) < 100)\n",
    "                   \n",
    "    return good_trips, total_records - good_trips.count()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q2 answer\n",
    "# good_trips, num_bad = q2(sampled_rdd, total_records)\n",
    "good_trips, num_bad = q2(taxi_rdd, total_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bad / total_records\n",
    "_, good_trips_by_year = q1(good_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167633\n",
      "8.38165\n"
     ]
    }
   ],
   "source": [
    "print(num_bad)\n",
    "print(num_bad / total_records *100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2013', 250767),\n",
       " ('2014', 344979),\n",
       " ('2018', 204488),\n",
       " ('2015', 297365),\n",
       " ('2017', 243462),\n",
       " ('2016', 299756),\n",
       " ('2020', 28431),\n",
       " ('2019', 163119)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_trips_by_year.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2018_rides(rdd):\n",
    "    return rdd.filter(lambda x: getattr(x, 'Trip Start Timestamp').split('/')[-1].split(' ')[0] == '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_2018 = get_2018_rides(good_trips)\n",
    "\n",
    "# https://spark.apache.org/docs/latest/rdd-programming-guide.html#external-datasets\n",
    "# Save/ Load 2018 data to avoid having to recreate from scratch\n",
    "rides_2018.saveAsPickleFile('2018_rdd')\n",
    "# rides_2018_df = rides_2018.toDF()\n",
    "# rides_2018 = sc.pickleFile('2018_rdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tricky because of custom aggregation required by excluding tolls\n",
    "# def q3(df):\n",
    "#     df.groupBy('Taxi Id').agg({'Total Price':\"avg\"}).orderBy('column_name', ascending=False)\n",
    "\n",
    "\n",
    "# MapReduce approach - not complete\n",
    "def q3(rdd):\n",
    "    \"\"\"For each taxi, calculate the average revenue per day excluding tolls (i.e. Fare + Tips). \n",
    "    Identify the most successful taxi in 2018 in terms of total revenue (Fare + Tips).\n",
    "    \n",
    "    https://stackoverflow.com/questions/29930110/calculating-the-averages-for-each-key-in-a-pairwise-k-v-rdd-in-spark-with-pyth\n",
    "    \n",
    "    \"\"\"\n",
    "    return rdd.map(lambda x: (get(x, 'Taxi ID'), [get(x, 'Fare') + get(x, 'Tips'), 1])\n",
    "                      ).reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])).mapValues(\n",
    "        lambda v: v[0]/v[1]).takeOrdered(6, key=lambda x: -x[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q3 answer\n",
    "sorted_fares = q3(rides_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204488"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides_2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q3_aggregation(sorted_fares, id_='50b668c005b90b8a98cb429f7ad632b913158b885e8c0a2948c4ed8a39801ca3027d4b0e3ee313f82046c085dd7ae8b044666fbd612e0ef663700efbf1dcc54a'):\n",
    "    \"\"\"Test to verify that aggregation logic is correct for q3 is correct using simple python\"\"\"\n",
    "    fares = rides_2018.filter(lambda x: get(x, 'Taxi ID') == id_).map(lambda x: [get(x, 'Fare') + get(x, 'Tips'), 1]).collect()\n",
    "\n",
    "    acc = 0\n",
    "    for x in fares:\n",
    "        acc += x[0]\n",
    "    assert acc / len(ble) == sorted_fares.filter(lambda x: x[0] == id_).collect()[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q4\n",
    "\n",
    "def prepare_q4(rdd):\n",
    "    \"\"\" Taking 1 hour periods throughout the day (from midnight to midnight) \n",
    "    across the complete dataset, answer the following. \n",
    "    Where a trip crosses a boundary (where the drop off is in a different period to the pickup),\n",
    "    assign that trip to the period where the midpoint of the journey happened.\n",
    "    \n",
    "    a. What is the average speed of taxis during each period?\n",
    "    b. Which is the period where drivers in total earn the most money in\n",
    "    terms of fares?\n",
    "    c. Which is the period of the day where drivers in total earn the most\n",
    "    in tips?\n",
    "    \n",
    "    \n",
    "    Approach: \n",
    "    \n",
    "    - Find the create a tuple of start and end-times (S, E)\n",
    "    - Convert each of these into DateTime instances \n",
    "    - Find the midpoint, that is the start, + the time delta of the end - start, \n",
    "    and then set the hour in scope to this hour.\n",
    "    \n",
    "    \n",
    "    N.b. due to not being able to assign and therefore reuse variables in the context of the lambda func, \n",
    "    the start time hs to to computed twice in this implementation. Whilst the code is very concise and expressive, \n",
    "    however it is slightly inefficent. A possible refactor is to use a function which takes a row rather than the \n",
    "    whole RDD and map to this.\n",
    "        \n",
    "    \"\"\"    \n",
    "\n",
    "    def midpoint(x): \n",
    "        \"\"\"Midpoint:  lambda x: (x[0] + (x[1] - x[0]) /2).hour) , where x = (start, end) \"\"\"\n",
    "        start = string_to_time(get(x, 'Trip Start Timestamp'))\n",
    "        end = string_to_time(get(x, 'Trip End Timestamp'))\n",
    "        return start, end, (start + (start - end) /2).hour\n",
    "    \n",
    "    \n",
    "    def avg_speed(x): \n",
    "        try: \n",
    "            return ((get(x, 'Trip Miles') / (get(x, 'Trip Seconds'))) * 60) * 60\n",
    "        except DivideByZeroError:\n",
    "        # N.b. should be neccasary as bad data already filtered out, \n",
    "        # but this makes the function safer for any input\n",
    "            return 0\n",
    "\n",
    "    Prepared = namedtuple('Prepared', ['fare', 'tips', 'avg_speed', 'start', 'end', 'midpoint', 'miles'])\n",
    "    return rdd.map(lambda x: Prepared(get(x, 'Fare'), get(x, 'Tips'), avg_speed(x), \n",
    "                                       *midpoint(x), get(x, 'Trip Miles')))\n",
    "                                        \n",
    "\n",
    "def q4(df):\n",
    "    \"\"\" Find the max values for the prepared df\n",
    "    \n",
    "    \"\"\"\n",
    "    #  Various permuations of this implemention were theorised, such as doing as:\n",
    "    \n",
    "    # 1. pure sparksql implemention as below:\n",
    "    # speedy = hourly_avgs.agg(max('avg_speed'))\n",
    "    #     tips = hourly_avgs.agg(max('tips'))\n",
    "    #     fares = hourly_avgs.agg(max('fare'))\n",
    "    #     return hourly_avgs, tips, fares\n",
    "\n",
    "    # however, this leads to greater complexity as each each .agg call returns just the datapoint, not the row \n",
    "    # i.e. cruically the midpoint is lost\n",
    "\n",
    "    # 2. A simple sort for each feature and then take the first item of each rdd.\n",
    "    # This was rejected as it is impossible to avoid three sort operations which are not only expense operations\n",
    "    # in general, but requires wide dependencies reduction, so even more expensive here.\n",
    "\n",
    "\n",
    "    # 3. A transformation based approach:\n",
    "\n",
    "    # Transfor the row data into k, v tuples for the relevent features \n",
    "    # and then find the max of each of these\n",
    "\n",
    "    #  e.g. transform a given row rdd of format [m, s, f, t] to: \n",
    "    # [(m,s), (m,f), (m,t)]\n",
    "    # This would enable calling performing a filter on each position to gain the maxium averages for each parameter.\n",
    "    \n",
    "    hourly_avgs = df.groupBy('midpoint').agg(avg('avg_speed'), avg('fare'), avg('tips'))\n",
    "    rdd = hourly_avgs.rdd\n",
    "    \n",
    "    \n",
    "    Results = namedtuple('Results', ['midpoint', 'fare', 'tips', 'avg_speed'])\n",
    "    results = rdd.map(lambda x: Results(x.midpoint, x[1], x[2], x[3]))\n",
    "    \n",
    "#   N.b. `or 0` handles comparision of NoneTypes as part of the max function   \n",
    "    max_fare = results.max(key=lambda x: x.fare or 0)\n",
    "    max_tips = results.max(key=lambda x: x.tips or 0)\n",
    "    max_avg_speed = results.max(key=lambda x: x.avg_speed or 0 )\n",
    "    \n",
    "    return max_fare, max_tips, max_avg_speed\n",
    "\n",
    "#   https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKeyLocally\n",
    "#     return rdd_.reduceByKeyLocally(max).items()\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q4_mid_points():\n",
    "\n",
    "    start = string_to_time('04/13/2017 07:30:00 AM')\n",
    "    end = string_to_time('04/13/2017 07:37:00 AM')\n",
    "    assert (start + (end - start)/2).hour == 7\n",
    "    \n",
    "    start = string_to_time('04/13/2017 09:30:00 AM')\n",
    "    end = string_to_time('04/13/2017 07:37:00 AM')\n",
    "    assert (start + (end - start)/2).hour == 14\n",
    "    \n",
    "    start = string_to_time('04/13/2017 11:30:00 PM')\n",
    "    end = string_to_time('04/18/2017 01:00:00 AM')\n",
    "    assert (start + (end - start)/2).hour == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 17 in stage 18.0 failed 4 times, most recent failure: Lost task 17.3 in stage 18.0 (TID 293, 172.31.7.245, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-24-eb8da8c0f5e3>\", line 48, in <lambda>\n  File \"<ipython-input-24-eb8da8c0f5e3>\", line 35, in midpoint\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-24-eb8da8c0f5e3>\", line 48, in <lambda>\n  File \"<ipython-input-24-eb8da8c0f5e3>\", line 35, in midpoint\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-bdb7873cd252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# prepared_rdd.take(5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprepared_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepared_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmax_fare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tips\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_avg_speed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-eb8da8c0f5e3>\u001b[0m in \u001b[0;36mq4\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m#   N.b. `or 0` handles comparision of NoneTypes as part of the max function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mmax_fare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfare\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mmax_tips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtips\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mmax_avg_speed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_speed\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mmax\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 17 in stage 18.0 failed 4 times, most recent failure: Lost task 17.3 in stage 18.0 (TID 293, 172.31.7.245, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-24-eb8da8c0f5e3>\", line 48, in <lambda>\n  File \"<ipython-input-24-eb8da8c0f5e3>\", line 35, in midpoint\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-24-eb8da8c0f5e3>\", line 48, in <lambda>\n  File \"<ipython-input-24-eb8da8c0f5e3>\", line 35, in midpoint\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# answer q4\n",
    "# prepared_rdd = prepare_q4(rides_2018)\n",
    "# prepared_rdd.first()\n",
    "\n",
    "# prepared_rdd.take(5)\n",
    "prepared_df = prepared_rdd.toDF()\n",
    "max_fare, max_tips, max_avg_speed = q4(prepared_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_q5(prepared_rdd):\n",
    "    \"\"\" What is the overall percentage of tips that drivers get?\n",
    "    Find the top ten trips with the best tip per distance travelled. \n",
    "\n",
    "    Create a graph of average tip percentage by month for the whole period.\n",
    "    \n",
    "    \"\"\"\n",
    "    def tips_percentage(row): \n",
    "        return (row.tips / row.fare) * 100\n",
    "    \n",
    "    def tip_per_mile(row):\n",
    "        try:\n",
    "            return row.tips / row.miles\n",
    "        # In the case of a trip of 0 miles, just use the tip amount\n",
    "        except ZeroDivisionError:\n",
    "            return row.tips\n",
    "      \n",
    "    \n",
    "    \n",
    "    Q5Results = namedtuple('Q5Results', ['start', 'month', 'fare', 'tips', 'tip_per_mile', 'tip_percentage_of_fare'])\n",
    "    return prepared_rdd.map(lambda x: Q5Results(x.start, calendar.month_name[x.start.month], x.fare, x.tips, \n",
    "                                                                   tip_per_mile(x), tips_percentage(x))\n",
    "                                               ).sortBy(lambda x: -x.tip_per_mile)\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_overall_tips_percentage(prepared_rdd):\n",
    "       return prepared_rdd.map(lambda x: x.tips.sum() / prepared_rdd.map(lambda x: x.fare).sum() * 100)\n",
    "    \n",
    "\n",
    "def get_tips_percentage_per_month(prepared_rdd):\n",
    "       return prepared_rdd.sortBy(lambda x: x.start)\n",
    "    \n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 answer\n",
    "prepared_q5_rdd = prepare_q5(prepared_rdd)\n",
    "generous_tippers = prepared_q5_rdd.take(10)\n",
    "tippers_by_month = prepared_q5_rdd.sortBy(lambda x: x.start).take(10)\n",
    "\n",
    "\n",
    "df =  prepared_q5_rdd.sortBy(lambda x: x.start).toDF().toPandas()\n",
    "avg = df.groupby('month').mean()\n",
    "\n",
    "\n",
    "# Q5 Plot\n",
    "figure, axes = plt.subplots(1,1)\n",
    "# raw_plt = axes.bar(df['start'], df['tip_percentage_of_fare'])\n",
    "avg_plt = axes.bar(avg.index, avg['tip_percentage_of_fare'])\n",
    "\n",
    "\n",
    "axes.set_title('Tip Percentge of total fare per month', fontsize=20)\n",
    "axes.set_xlabel('Month')\n",
    "axes.set_ylabel('Tip Percentge')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6\n",
    "\n",
    "# N.b. Lat and Long chosen over the Location data as this would reqiure further parsing\n",
    "\n",
    "def prepare_geo_data(rdd):\n",
    "    \n",
    "    \n",
    "    Q6Results = namedtuple('Q6Results', ['start_lat', 'start_long',\n",
    "                                         'end_lat', 'end_long',\n",
    "                                         'start_timestamp'])\n",
    "    return rdd.map(lambda x: Q6Results(get(x, 'Pickup Centroid Latitude'), \n",
    "                   get(x, 'Pickup Centroid Longitude'),\n",
    "                   get(x, 'Dropoff Centroid Latitude'), \n",
    "                   get(x, 'Dropoff Centroid Longitude'),\n",
    "                   string_to_time(get(x, 'Trip Start Timestamp'))))\n",
    "\n",
    "\n",
    "# https://spark.apache.org/docs/latest/ml-clustering.html\n",
    "# https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.clustering.KMeans\n",
    "# https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler\n",
    "def q6(df):\n",
    "    import pandas as pd\n",
    "    from pyspark.ml.clustering import KMeans\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "    \n",
    "    vectors = VectorAssembler(inputCols=['start_lat', 'start_long'], \n",
    "                              outputCol='features', handleInvalid='skip')\n",
    "    df_ = vectors.transform(df)\n",
    "\n",
    "    kmeans = KMeans(k=10, seed=1)\n",
    "    model = kmeans.fit(df_.select('features'))\n",
    "    predictions = model.transform(df_)\n",
    "    centers = model.clusterCenters()\n",
    "    \n",
    "    predictions.centers = pd.Series(centers)\n",
    "    \n",
    "#     evaluator = ClusteringEvaluator()\n",
    "#     silhouette = evaluator.evaluate(predictions)\n",
    "#     print(f'Silhouette with squared euclidean distance = {str(silhouette)}')\n",
    "    \n",
    "    print('Cluster Centers: ')\n",
    "    for center in centers:\n",
    "        print(center)\n",
    "          \n",
    "          \n",
    "    return predictions, centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer q6\n",
    "prepared_geo_data = prepare_geo_data(rides_2018)\n",
    "predictions, centers = q6(prepared_geo_data.toDF())\n",
    "q6_answer = predictions.groupBy('prediction', 'start_lat', 'start_long').count().orderBy(\n",
    "    'count', ascending=False)\n",
    "prepared_geo_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepared_geo_data.saveAsPickleFile('prepared_geo_data')\n",
    "prepared_geo_data = sc.pickleFile('prepared_geo_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q6_answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-24bab5140714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq6_answer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# df.plot(kind='scatter', x='start_long', y='start_lat', alpha=0.4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q6_answer' is not defined"
     ]
    }
   ],
   "source": [
    "# plot q6\n",
    "\n",
    "# https://www.bigendiandata.com/2017-06-27-Mapping_in_Jupyter/\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "df = q6_answer.toPandas()\n",
    "\n",
    "# df.plot(kind='scatter', x='start_long', y='start_lat', alpha=0.4)\n",
    "# plt.show()\n",
    "\n",
    "chicago_img=mpimg.imread('/home/ec2-user/chicago.png')\n",
    "\n",
    "\n",
    "axes = df.plot(kind=\"scatter\", x=\"start_long\", y=\"start_lat\",\n",
    "    s=df['count'] *100, label=\"count\",\n",
    "               cmap=plt.get_cmap(\"jet\"),\n",
    "#                colorbar=True, \n",
    "               alpha=0.4, figsize=(10,7)\n",
    ")\n",
    "\n",
    "plt.imshow(chicago_img, alpha=0.5,\n",
    "           interpolation='nearest' )\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('samples in cluster', fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lat = df.start_lat.min()\n",
    "max_lat = df.start_lat.max()\n",
    "min_long = df.start_long.min()\n",
    "max_long = df.start_long.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download image\n",
    "# CHICAGO_URL ='https://www.openstreetmap.org/#map=10/41.6226/-87.7231'\n",
    "def download_image():\n",
    "    CHICAGO_URL = 'https://osm.org/go/YXb1'\n",
    "    import requests\n",
    "    r = requests.get(CHICAGO_URL, allow_redirects=True)\n",
    "    open('chicago.png', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.bigendiandata.com/2017-06-27-Mapping_in_Jupyter/\n",
    "%matplotlib inline\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "import pandas as pd\n",
    "coords = pd.concat([df['start_lat'], df['start_long'], df['count']], axis=1)\n",
    "coords = coords.sample(frac=0.1, replace=True)\n",
    "cases = []\n",
    "# colors = ['rgb(239,243,255)','rgb(189,215,231)','rgb(107,174,214)','rgb(33,113,181)']\n",
    "\n",
    "for i in range(6,10)[::-1]:\n",
    "    cases.append(go.Scattergeo(\n",
    "        lon = coords['start_lat'],\n",
    "        lat = coords['start_long'],\n",
    "        marker = dict(\n",
    "            size = coords['count'] * 100,\n",
    "            color = 'darkolivegreen',\n",
    "            opacity = .4,\n",
    "            line = dict(width = 0)\n",
    "        ),\n",
    "    ) )\n",
    "\n",
    "cases[0]['mode'] = 'markers'\n",
    "\n",
    "layout = go.Layout(\n",
    "    geo = dict(\n",
    "        resolution = 110,\n",
    "        scope = 'usa',\n",
    "        showframe = False,\n",
    "        showcoastlines = True,\n",
    "        showland = True,\n",
    "        landcolor = \"rgb(229, 229, 229)\",\n",
    "        countrycolor = \"rgb(255, 255, 255)\" ,\n",
    "        coastlinecolor = \"rgb(255, 255, 255)\",\n",
    "        projection = dict(\n",
    "            type = 'mercator'\n",
    "        ),\n",
    "        lonaxis = dict( range= [min_long, max_long] ),\n",
    "        lataxis = dict( range= [ min_lat, max_lat] ),\n",
    "        \n",
    "    ),\n",
    "    legend = dict(\n",
    "           traceorder = 'reversed'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig = go.Figure(layout=layout, data=cases)\n",
    "# plotly.offline.iplot(fig, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7\n",
    "zip_seacher = ZipCodeEngine(simple_zipcode=True)\n",
    "\n",
    "\n",
    "def map_postcodes(row):\n",
    "    \"\"\"Return a list of tuples of zipcodes for a given trip's start end coords. \n",
    "\n",
    "    N.b. a coordinate can map to >1 postcode, (by default uszipcode returns 5 addresses per lat/long search),\n",
    "    however sometimes these results return duplicates. In order to dedupe the results the following method is used:\n",
    "\n",
    "    0. construct a list of tuples of list of zipcodes pairs\n",
    "        0.a query the start_lat, start_long and end_lat, end_long coordinates\n",
    "        0.b assemble (start, end) pairs by zipping the results back\n",
    "\n",
    "    1. Cast zipcodes to ints, e.g at this point the data resembles: \n",
    "\n",
    "    [(60640, 60660),\n",
    "     (60660, 60640),\n",
    "     (60613, 60626),\n",
    "     (60657, 60659),\n",
    "     (60625, 60645)]\n",
    "     # - n.b. index 0,1 are essentially dupes with the start/end positions switched\n",
    "\n",
    "    2. sort the zipcode pairs \n",
    "    3. cast the output to a set to remove dupicate entries: \n",
    "\n",
    "    {(60613, 60626), (60625, 60645), (60640, 60660), (60657, 60659)}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # https://pypi.org/project/uszipcode/\n",
    "    starts = [res.zipcode for res in zip_seacher.by_coordinates(get(row, 'start_lat'), get(row, 'start_long'))]\n",
    "    ends = [res.zipcode for res in zip_seacher.by_coordinates(get(row, 'start_lat'), get(row, 'start_long'))]\n",
    "\n",
    "#     starts = [res.zipcode for res in zip_seacher.by_coordinates(start_lat, start_long)]\n",
    "#     ends = [res.zipcode for res in zip_seacher.by_coordinates(start_lat, start_long)]\n",
    "\n",
    "    return set([tuple(sorted([int(s), int(e)])) for s,e in zip(starts, ends)])\n",
    "\n",
    "\n",
    "def q7(rdd):\n",
    "    Q7Results = namedtuple('Q7Results', ['zipcodes'])\n",
    "    # TO DO finish this - currently errors, not sure why\n",
    "    return rdd.map(map_postcodes(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df, weather_rdd = get_data(sql_context, WEATHER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8\n",
    "def prepare_weather_data(rdd):\n",
    "    Q8Data = namedtuple('Q8Data', ['meaurement_id','rain_interval', 'intensity','station',  'timestamp', ])\n",
    "    return rdd.map(lambda x: Q8Data( get(x, 'Measurement ID'), get(x, 'Interval Rain'), \n",
    "                    get(x, 'Rain Intensity'),\n",
    "                    get(x, 'Station Name').replace(' ', ''),\n",
    "                    string_to_time(get(x, 'Measurement Timestamp')\n",
    "                                  )))\n",
    "                    # gladly TS format matches so this helper can be reused...\n",
    "\n",
    "def q8(q8_data):\n",
    "    \"\"\"Uses a similar approach to question 6, group the data then recast to RDD and \n",
    "    the max values. \"\"\"\n",
    "        \n",
    "    grouped = q8_data.toDF().groupby('timestamp').mean()\n",
    "    rdd = grouped.rdd\n",
    "\n",
    "\n",
    "    Results = namedtuple('Results', ['timestamp', 'avg_interval', 'avg_intensity'])\n",
    "    results = rdd.map(lambda x: Results(x.timestamp, x[1], x[2]))\n",
    "\n",
    "    #   N.b. `or 0` handles comparision of NoneTypes as part of the max function   \n",
    "    max_interval = results.max(key=lambda x: x.avg_interval or 0)\n",
    "    max_intensity = results.max(key=lambda x: x.avg_intensity or 0)\n",
    "\n",
    "    return max_interval, max_intensity, rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer q8\n",
    "q8_data = prepare_weather_data(weather_rdd)\n",
    "max_interval, max_intensity, rain_rdd = q8(q8_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9\n",
    "\n",
    "def prepare_q9(rain_data_df, taxi_data_df):\n",
    "    \"\"\"N.b. this function isn't generic - reqiures DF in correct format to be able to join. \n",
    "    rain_data_df - e.g. as output from `q8` \n",
    "    taxi_data_df - e.g. as output from `q4`\n",
    "    \n",
    "    \"\"\"\n",
    "    def to_unix_timestamp(isoformat):\n",
    "        \"\"\"Correlation not supported on timestamp data, so need to convert timestamps to ints.\n",
    "        \"\"\"\n",
    "        import time \n",
    "        return int(time.mktime(time.strptime(isoformat, '%Y-%m-%dT%H:%M:%S')))\n",
    "    \n",
    "    # https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=join\n",
    "    joined = rain_data_df.join(taxi_data_df, taxi_data_df.start == rain_data_df.timestamp)\n",
    "    \n",
    "    Q9Results = namedtuple('Q9Results', ['timestamp', 'avg_interval', 'avg_intensity', 'fare', 'tips'])\n",
    "    rdd = joined.rdd.map(lambda x: Q9Results(\n",
    "        to_unix_timestamp(x.timestamp.isoformat()),\n",
    "        get(x, 'avg(rain_interval)'), get(x, 'avg(intensity)'),\n",
    "        x.fare, x.tips))\n",
    "    return rdd\n",
    "\n",
    "def q9(df):\n",
    "    \n",
    "#     # https://spark.apache.org/docs/2.2.0/ml-statistics.html#correlation\n",
    "#     from pyspark.ml.stat import Correlation\n",
    "#     from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#     # convert to vector column first\n",
    "#     assembler = VectorAssembler(inputCols=df.columns, outputCol=vector_col, handleInvalid='skip')\n",
    "#     df_vector = assembler.transform(df).select('features')\n",
    "\n",
    "    # https://people.eecs.berkeley.edu/~jegonzal/pyspark/pyspark.sql.html#pyspark.sql.DataFrame.corr\n",
    "    fare_corr = df.stat.corr(\"timestamp\", \"fare\")\n",
    "    tip_corr = df.stat.corr(\"timestamp\", \"tips\")\n",
    "    return fare_corr, tip_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q9_data = prepare_q9(rain_rdd.toDF(), prepared_rdd.toDF())\n",
    "fare_corr, tip_corr = q9(q9_data.toDF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8407364102310766"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fare_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

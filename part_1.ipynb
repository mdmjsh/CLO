{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from datetime import datetime\n",
    "\n",
    "BIG_TAXI = 's3a://chictaxi/chictaxi.csv'\n",
    "SMALL_TAXI = 's3a://chictaxi/small.csv'\n",
    "WEATHER = 's3a://chictaxi/weather.csv'\n",
    "\n",
    "# sc = SparkContext()\n",
    "sql_context = SQLContext(sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sql_context, path=SMALL_TAXI):\n",
    "    df = sql_context.read.csv(path, header='true', inferSchema='true')\n",
    "    return (df, df.rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df, taxi_rdd = get_data(sql_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=rdd#pyspark.RDD.sample \n",
    "sampled_rdd = taxi_rdd.sample(False, 0.0001, 81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1(rdd):\n",
    "    \"\"\"How many taxi records are there? \n",
    "    How many taxi records for each year of the dataset?\n",
    "    \"\"\"\n",
    "    count = rdd.count() \n",
    "    yearly_counts = rdd.map(lambda x: (getattr(x, 'Trip Start Timestamp'\n",
    "                                              ).split('/')[-1].split(' ')[0], 1)).reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "    return count, yearly_counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1 answer\n",
    "total_records, yearly_counts = q1(sampled_rdd)\n",
    "# yearly_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(x, key, default=0):\n",
    "    return getattr(x, key) or default\n",
    "\n",
    "def q2(rdd, total_records):\n",
    "    \"\"\"\"How many records in total would you classify as bad? \n",
    "        Consider a bad record to be one where the Trip Seconds are less than 60,\n",
    "        but also if the average speed is over 100 mph, the distance is more \n",
    "        than 1000 miles or the fare is over $2000 (excluding tips, tolls, etc). \n",
    "        \n",
    "        Once you have defined this, ensure that all further answers are based only on good data. \n",
    "        How many records are “good” by year\n",
    "        \n",
    "        \n",
    "        N.b. for trips under 1 mile, the ave. speed = 0.0 (as miles = 0), therefore this is a decent approximation\n",
    "        without guarenteeing total accuracy as it doesn't take into account the precise coordinates of the journey\n",
    "        when calculating average speed.\n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    good_trips = rdd.filter(lambda x: (get(x, 'Trip Seconds') > 60 )\n",
    "                    & (get(x, 'Trip Miles') < 1000)\n",
    "                    & (get(x, 'Fare') < 2000))\n",
    "#                     & ((getattr(x, 'Trip Miles', 0) / (getattr(x, 'Trip Seconds', 0) / 60)) < 100)\n",
    "                   \n",
    "    return good_trips, total_records - good_trips.count()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q2 answer\n",
    "good_trips, num_bad = q2(sampled_rdd, total_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2018_rides(rdd):\n",
    "    return rdd.filter(lambda x: getattr(x, 'Trip Start Timestamp').split('/')[-1].split(' ')[0] == '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_2018 = get_2018_rides(good_trips)\n",
    "# rides_2018_df = rides_2018.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tricky because of custom aggregation required by excluding tolls\n",
    "# def q3(df):\n",
    "#     df.groupBy('Taxi Id').agg({'Total Price':\"avg\"}).orderBy('column_name', ascending=False)\n",
    "\n",
    "\n",
    "# MapReduce approach - not complete\n",
    "def q3(rdd):\n",
    "    \"\"\"For each taxi, calculate the average revenue per day excluding tolls (i.e. Fare + Tips). \n",
    "    Identify the most successful taxi in 2018 in terms of total revenue (Fare + Tips).\n",
    "    \n",
    "    https://stackoverflow.com/questions/29930110/calculating-the-averages-for-each-key-in-a-pairwise-k-v-rdd-in-spark-with-pyth\n",
    "    \n",
    "    \"\"\"\n",
    "    return rdd.map(lambda x: (get(x, 'Taxi ID'), [get(x, 'Fare') + get(x, 'Tips'), 1])\n",
    "                      ).reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])).mapValues(\n",
    "        lambda v: v[0]/v[1]).takeOrdered(6, key=lambda x: -x[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q3 answer\n",
    "sorted_fares = q3(rides_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q3_aggregation(sorted_fares, id_='50b668c005b90b8a98cb429f7ad632b913158b885e8c0a2948c4ed8a39801ca3027d4b0e3ee313f82046c085dd7ae8b044666fbd612e0ef663700efbf1dcc54a'):\n",
    "    \"\"\"Test to verify that aggregation logic is correct for q3 is correct using simple python\"\"\"\n",
    "    fares = rides_2018.filter(lambda x: get(x, 'Taxi ID') == id_).map(lambda x: [get(x, 'Fare') + get(x, 'Tips'), 1]).collect()\n",
    "\n",
    "    acc = 0\n",
    "    for x in fares:\n",
    "        acc += x[0]\n",
    "    assert acc / len(ble) == sorted_fares.filter(lambda x: x[0] == id_).collect()[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q4\n",
    "\n",
    "# q4\n",
    "\n",
    "def string_to_time(date):\n",
    "    \"\"\"E.g. turns '04/13/2017 07:30:00 AM' into datetime.time(6, 15).    \n",
    "\n",
    "    N.b. extra complexity here as time format isn't a simple 24hr clock;\n",
    "    first convert to PM times to 24 hr format by manipulating the string, \n",
    "    then convert to DateTime.\n",
    "    \"\"\"\n",
    "    if 'PM' in date:\n",
    "        time = date.split(' ')[1]\n",
    "        hour = str(int(date.split(':')[0].split(' ')[-1]) + 12)\n",
    "        _time = hour + time[2:]\n",
    "        _date = date.replace(time, _time)[:-3]\n",
    "        \n",
    "    else:\n",
    "        _date = date[:-3]\n",
    "    # https://www.journaldev.com/23365/python-string-to-datetime-strptime\n",
    "    return datetime.strptime(_date, '%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "\n",
    "def test_string_to_time():\n",
    "    assert string_to_time('04/13/2017 07:30:00 PM') == datetime.datetime(2017, 4, 13, 19, 30)\n",
    "    assert string_to_time('04/13/2017 07:30:00 AM') == datetime.datetime(2017, 4, 13, 7, 30)\n",
    "\n",
    "\n",
    "def q4(rdd):\n",
    "    \"\"\" Taking 1 hour periods throughout the day (from midnight to midnight) \n",
    "    across the complete dataset, answer the following. \n",
    "    Where a trip crosses a boundary (where the drop off is in a different period to the pickup),\n",
    "    assign that trip to the period where the midpoint of the journey happened.\n",
    "    \n",
    "    a. What is the average speed of taxis during each period?\n",
    "    b. Which is the period where drivers in total earn the most money in\n",
    "    terms of fares?\n",
    "    c. Which is the period of the day where drivers in total earn the most\n",
    "    in tips?\n",
    "    \n",
    "    \n",
    "    Approach: \n",
    "    \n",
    "    - Find the create a tuple of start and end-times (S, E)\n",
    "    - Convert each of these into DateTime instances \n",
    "    - Find the midpoint, that is the start, + the time delta of the end - start, \n",
    "    and then set the hour in scope to this hour.\n",
    "    \n",
    "    \n",
    "    N.b. due to not being able to assign and therefore reuse variables in the context of the lambda func, \n",
    "    the start time hs to to computed twice in this implementation. Whilst the code is very concise and expressive, \n",
    "    however it is slightly inefficent. A possible refactor is to use a function which takes a row rather than the \n",
    "    whole RDD and map to this.\n",
    "        \n",
    "    \"\"\"    \n",
    "    #   Midpoint:  lambda x: (x[0] + (x[1] - x[0]) /2).hour) , where x = (start, end)\n",
    "    \n",
    "    def start(x): return string_to_time(get(x, 'Trip Start Timestamp'))\n",
    "    def end(x): return string_to_time(get(x, 'Trip Start Timestamp'))\n",
    "    def avg_speed(x): \n",
    "        try: \n",
    "            return (get(x, 'Trip Miles') / (get(x, 'Trip Seconds'))) / 60\n",
    "        except DivideByZeroError:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    from collections import namedtuple\n",
    "    TripData = namedtuple('TripData', ['fare', 'tips', 'avg_speed'])\n",
    "    Midpoint = namedtuple('Midpoint', ['trip_data' , 'start', 'end', 'midpoint'])\n",
    "\n",
    "    \n",
    "    return rdd.map(lambda x: Midpoints(TripData(get(x, 'Fare'), get(x, 'Tips'), \n",
    "                                        avg_speed(x)), \n",
    "                                       start(x), +  (end(x) - start(x)) / 2).hour)\n",
    "                                        \n",
    "\n",
    "                                     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q4_mid_points():\n",
    "\n",
    "    start = string_to_time('04/13/2017 07:30:00 AM')\n",
    "    end = string_to_time('04/13/2017 07:37:00 AM')\n",
    "    assert (start + (end - start)/2).hour == 7\n",
    "    \n",
    "    start = string_to_time('04/13/2017 09:30:00 AM')\n",
    "    end = string_to_time('04/13/2017 07:37:00 AM')\n",
    "    assert (start + (end - start)/2).hour == 14\n",
    "    \n",
    "    start = string_to_time('04/13/2017 11:30:00 PM')\n",
    "    end = string_to_time('04/18/2017 01:00:00 AM')\n",
    "    assert (start + (end - start)/2).hour == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 4 times, most recent failure: Lost task 0.3 in stage 42.0 (TID 186, 172.31.12.34, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/spark/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-188-c1185248a845>\", line 49, in <lambda>\nAttributeError: 'datetime.datetime' object has no attribute 'map'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/spark/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-188-c1185248a845>\", line 49, in <lambda>\nAttributeError: 'datetime.datetime' object has no attribute 'map'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-dc84966839ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrides_2018\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/clo/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/clo/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 4 times, most recent failure: Lost task 0.3 in stage 42.0 (TID 186, 172.31.12.34, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/spark/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-188-c1185248a845>\", line 49, in <lambda>\nAttributeError: 'datetime.datetime' object has no attribute 'map'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/spark/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ec2-user/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-188-c1185248a845>\", line 49, in <lambda>\nAttributeError: 'datetime.datetime' object has no attribute 'map'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "re = q4(rides_2018)\n",
    "\n",
    "re.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "map",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1532\u001b[0m             \u001b[0;31m# but this will not be used in normal cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1533\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fields__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1534\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'map' is not in list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-7a42bba80d22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Trip Start Timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: map"
     ]
    }
   ],
   "source": [
    "x.map(lambda x: ((getattr(x, 'Trip Start Timestamp').split(' ')[0], 1)), (getattr(x, 'Trip End Timestamp').split(' ')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unconverted data remains:  AM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-81d9e5009be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Trip Start Timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%m/%d/%Y %H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib64/python3.6/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    564\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 565\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         raise ValueError(\"unconverted data remains: %s\" %\n\u001b[0;32m--> 365\u001b[0;31m                           data_string[found.end():])\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0miso_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unconverted data remains:  AM"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "datetime.strptime(getattr(x, 'Trip Start Timestamp'), '%m/%d/%Y %H:%M:%S').time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = string_to_time('06:15:00')\n",
    "end = string_to_time('06:18:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Expected an ISO 8601-like string, but was given '03/10/2015 11:15:00 PM'. Try passing in a format string to resolve this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-577c51aec87d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'03/10/2015 11:15:00 PM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/clo/lib/python3.6/site-packages/arrow/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/clo/lib/python3.6/site-packages/arrow/factory.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# (str) -> parse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDateTimeParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_iso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/clo/lib/python3.6/site-packages/arrow/parser.py\u001b[0m in \u001b[0;36mparse_iso\u001b[0;34m(self, datetime_string)\u001b[0m\n\u001b[1;32m    124\u001b[0m             raise ParserError(\n\u001b[1;32m    125\u001b[0m                 \"Expected an ISO 8601-like string, but was given '{}'. Try passing in a format string to resolve this.\".format(\n\u001b[0;32m--> 126\u001b[0;31m                     \u001b[0mdatetime_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 )\n\u001b[1;32m    128\u001b[0m             )\n",
      "\u001b[0;31mParserError\u001b[0m: Expected an ISO 8601-like string, but was given '03/10/2015 11:15:00 PM'. Try passing in a format string to resolve this."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Trip ID='180cc29b289f9fa2680cf6baec647b0acb488a35', Taxi ID='d5c4fbae1c0c510364404a90fd477b19f7f03408ce40ff2cdb76b991835eadc1d7b1540d939262f561ba227da02a2c7bbbb1fc093511af5d261f29e34cd76cdf', Trip Start Timestamp='04/13/2017 07:30:00 AM', Trip End Timestamp='04/13/2017 07:30:00 AM', Trip Seconds=300, Trip Miles=0.0, Pickup Census Tract=17031081000, Dropoff Census Tract=17031081800, Pickup Community Area=8, Dropoff Community Area=8, Fare=7.0, Tips=2.0, Tolls=0.0, Extras=0.0, Trip Total=9.0, Payment Type='Credit Card', Company='Taxi Affiliation Services', Pickup Centroid Latitude=41.900265687, Pickup Centroid Longitude=-87.63210922, Pickup Centroid Location='POINT (-87.6321092196 41.9002656868)', Dropoff Centroid Latitude=41.89321636, Dropoff Centroid Longitude=-87.63784421, Dropoff Centroid  Location='POINT (-87.6378442095 41.8932163595)'),\n",
       " Row(Trip ID='bd8db45e120846d962d5505519e8cff54960de08', Taxi ID='65682911fa6de72195c92564114885bc4f8d8c17d5b00569a127edd4d8b0778bfc7984dc1078eeee048f600fa25efa8a89f0c12dea3b110b108f236cc1c01c32', Trip Start Timestamp='07/09/2017 12:00:00 PM', Trip End Timestamp='07/09/2017 12:45:00 PM', Trip Seconds=2160, Trip Miles=17.7, Pickup Census Tract=17031980000, Dropoff Census Tract=17031081403, Pickup Community Area=76, Dropoff Community Area=8, Fare=45.0, Tips=15.0, Tolls=0.0, Extras=4.0, Trip Total=64.0, Payment Type='Credit Card', Company='Top Cab Affiliation', Pickup Centroid Latitude=41.97907082, Pickup Centroid Longitude=-87.903039661, Pickup Centroid Location='POINT (-87.9030396611 41.9790708201)', Dropoff Centroid Latitude=41.890922026, Dropoff Centroid Longitude=-87.618868355, Dropoff Centroid  Location='POINT (-87.6188683546 41.8909220259)'),\n",
       " Row(Trip ID='c0ee852df9c3b2d5f8b5ce333774ce1a924141a7', Taxi ID='48c6cef6da6b6d321cc3f3781fbc4ac07a156f25c66f53dca399045aa08ff4dee5721ef8c27cf991d305a4fb00f5f969142764cf39c1219244fb053692c31147', Trip Start Timestamp='03/10/2015 11:00:00 PM', Trip End Timestamp='03/10/2015 11:15:00 PM', Trip Seconds=480, Trip Miles=1.8, Pickup Census Tract=17031280100, Dropoff Census Tract=17031081201, Pickup Community Area=28, Dropoff Community Area=8, Fare=7.25, Tips=1.0, Tolls=0.0, Extras=1.5, Trip Total=9.75, Payment Type='Credit Card', Company='Dispatch Taxi Affiliation', Pickup Centroid Latitude=41.885300022, Pickup Centroid Longitude=-87.642808466, Pickup Centroid Location='POINT (-87.6428084655 41.8853000224)', Dropoff Centroid Latitude=41.899155613, Dropoff Centroid Longitude=-87.626210532, Dropoff Centroid  Location='POINT (-87.6262105324 41.8991556134)'),\n",
       " Row(Trip ID='65fde2c15c4f211e7774e1bd8e1ff78cd6a5347b', Taxi ID='d8b304677754eacdf9fbfa9a1a2fc12375226b66e62cd5aa11cc66f8133d37c65f69693b66f1d66df1ff03f7dc58338e6fdc6e37d377994ff045e7f21e65d619', Trip Start Timestamp='01/14/2013 06:15:00 PM', Trip End Timestamp='01/14/2013 06:30:00 PM', Trip Seconds=300, Trip Miles=0.0, Pickup Census Tract=17031839100, Dropoff Census Tract=17031081800, Pickup Community Area=32, Dropoff Community Area=8, Fare=5.25, Tips=0.0, Tolls=0.0, Extras=0.0, Trip Total=5.25, Payment Type='Cash', Company='Taxi Affiliation Services', Pickup Centroid Latitude=41.880994471, Pickup Centroid Longitude=-87.632746489, Pickup Centroid Location='POINT (-87.6327464887 41.8809944707)', Dropoff Centroid Latitude=41.89321636, Dropoff Centroid Longitude=-87.63784421, Dropoff Centroid  Location='POINT (-87.6378442095 41.8932163595)'),\n",
       " Row(Trip ID='76a134c6bb54c08da5e71c47ecc0b93a2b3a8b09', Taxi ID='2bfab5f50d7ac81d00e074e342a1f90fdaf9d3f089e1113d155b36cf8eff013e6077e7166fbed11295ee60ccf7e0af143bb32e97518c78687f3dee79fadec20c', Trip Start Timestamp='12/04/2014 10:15:00 PM', Trip End Timestamp='12/04/2014 10:45:00 PM', Trip Seconds=2276, Trip Miles=17.2, Pickup Census Tract=None, Dropoff Census Tract=None, Pickup Community Area=8, Dropoff Community Area=None, Fare=37.65, Tips=0.0, Tolls=None, Extras=0.0, Trip Total=37.65, Payment Type='Cash', Company='Yellow Cab', Pickup Centroid Latitude=41.899602111, Pickup Centroid Longitude=-87.633308037, Pickup Centroid Location='POINT (-87.6333080367 41.899602111)', Dropoff Centroid Latitude=None, Dropoff Centroid Longitude=None, Dropoff Centroid  Location=None),\n",
       " Row(Trip ID='58c1e0f1db6a863fa977a7e48ad7adec25033641', Taxi ID='e02c769e58798617839ff61584e3f9a88086b0101cfed43797fcacf655e77628ac5d3adbc527340ebdd1e7fb43eeac29e2705f1b7cd52bcd5159a25e09f72fc2', Trip Start Timestamp='06/30/2014 07:30:00 PM', Trip End Timestamp='06/30/2014 07:45:00 PM', Trip Seconds=900, Trip Miles=4.0, Pickup Census Tract=None, Dropoff Census Tract=None, Pickup Community Area=16, Dropoff Community Area=5, Fare=11.85, Tips=0.0, Tolls=0.0, Extras=0.0, Trip Total=11.85, Payment Type='Cash', Company='Northwest Management LLC', Pickup Centroid Latitude=41.953582125, Pickup Centroid Longitude=-87.72345239, Pickup Centroid Location='POINT (-87.7234523905 41.9535821253)', Dropoff Centroid Latitude=41.947791586, Dropoff Centroid Longitude=-87.683834942, Dropoff Centroid  Location='POINT (-87.6838349425 41.9477915865)'),\n",
       " Row(Trip ID='b53ba61e3dff5df82a9f2141cf8c64f344a00c7d', Taxi ID='81a604cff41feea3ba5b61d8ec2bbc2483f73fd18351d05112c6c404d6c6098c3346c7e3dc97498bc66ffb19a2fc1ac531f9f625865f9c7a7d3ec7cd3898cf79', Trip Start Timestamp='03/15/2014 06:30:00 AM', Trip End Timestamp='03/15/2014 06:30:00 AM', Trip Seconds=180, Trip Miles=0.9, Pickup Census Tract=None, Dropoff Census Tract=None, Pickup Community Area=6, Dropoff Community Area=6, Fare=5.05, Tips=0.0, Tolls=0.0, Extras=0.0, Trip Total=5.05, Payment Type='Cash', Company='Taxi Affiliation Services', Pickup Centroid Latitude=41.944226601, Pickup Centroid Longitude=-87.655998182, Pickup Centroid Location='POINT (-87.6559981815 41.9442266014)', Dropoff Centroid Latitude=41.944226601, Dropoff Centroid Longitude=-87.655998182, Dropoff Centroid  Location='POINT (-87.6559981815 41.9442266014)'),\n",
       " Row(Trip ID='ec1c65543b3c67b7d9708782443e25c7f915c4f3', Taxi ID='3f57eddca4d626a05c7255c32e4e6134f17d7f331073b173675438e3a1f640e107e367f52c5702d2c45e1ac323d7fa47553673637213cb1f84bcc7dc1d098501', Trip Start Timestamp='01/24/2017 07:15:00 AM', Trip End Timestamp='01/24/2017 07:30:00 AM', Trip Seconds=840, Trip Miles=3.4, Pickup Census Tract=17031081500, Dropoff Census Tract=17031081700, Pickup Community Area=8, Dropoff Community Area=8, Fare=12.0, Tips=2.5, Tolls=0.0, Extras=0.0, Trip Total=15.0, Payment Type='Credit Card', Company='Medallion Leasin', Pickup Centroid Latitude=41.892507781, Pickup Centroid Longitude=-87.626214906, Pickup Centroid Location='POINT (-87.6262149064 41.8925077809)', Dropoff Centroid Latitude=41.892042136, Dropoff Centroid Longitude=-87.63186395, Dropoff Centroid  Location='POINT (-87.6318639497 41.8920421365)'),\n",
       " Row(Trip ID='7b828cc2530b13480f1e2545e230e203371f7987', Taxi ID='ab6b0472a82102ae6875f5297519510f5664a3b4a3559e7263a5040af7121af771246f9524f35f264ce10c26187e70f96d230f1df3362cca75b7d91b12b27148', Trip Start Timestamp='08/19/2019 11:00:00 AM', Trip End Timestamp='08/19/2019 11:00:00 AM', Trip Seconds=0, Trip Miles=0.0, Pickup Census Tract=17031840300, Dropoff Census Tract=17031840300, Pickup Community Area=59, Dropoff Community Area=59, Fare=3.25, Tips=0.0, Tolls=0.0, Extras=1.0, Trip Total=4.25, Payment Type='Cash', Company='Taxi Affiliation Services', Pickup Centroid Latitude=41.833517886, Pickup Centroid Longitude=-87.681355829, Pickup Centroid Location='POINT (-87.6813558293 41.8335178865)', Dropoff Centroid Latitude=41.833517886, Dropoff Centroid Longitude=-87.681355829, Dropoff Centroid  Location='POINT (-87.6813558293 41.8335178865)'),\n",
       " Row(Trip ID='33996c55b0f902a6502ca391e53880f766a23db8', Taxi ID='3506f62ee1c70c02fe00b742679684b319d5b712182f828f14b081888056485c3bd5911f60c194bcf0fb5986bb4f04b7d169ab0fd22cb63f0906af16d431395b', Trip Start Timestamp='04/26/2019 12:00:00 AM', Trip End Timestamp='04/26/2019 12:00:00 AM', Trip Seconds=180, Trip Miles=0.0, Pickup Census Tract=17031839100, Dropoff Census Tract=17031320100, Pickup Community Area=32, Dropoff Community Area=32, Fare=4.75, Tips=0.0, Tolls=0.0, Extras=0.0, Trip Total=4.75, Payment Type='Cash', Company='Taxi Affiliation Services', Pickup Centroid Latitude=41.880994471, Pickup Centroid Longitude=-87.632746489, Pickup Centroid Location='POINT (-87.6327464887 41.8809944707)', Dropoff Centroid Latitude=41.884987192, Dropoff Centroid Longitude=-87.620992913, Dropoff Centroid  Location='POINT (-87.6209929134 41.8849871918)')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04/13/2017 07:30:00'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'04/13/2017 07:30:00 AM'[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}